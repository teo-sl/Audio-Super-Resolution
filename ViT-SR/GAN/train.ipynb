{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ./drive/MyDrive/vit_sr/pickled.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftshift\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import cv2\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig, AdamW\n",
    "\n",
    "import librosa\n",
    "import librosa.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(3,3), padding=1, stride=2), \n",
    "                        nn.GELU(), \n",
    "                        nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(3,3), padding=1, stride=1)\n",
    "        ).to(device) \n",
    "        \n",
    "        self.ext_block = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(3,3), padding=1, stride=2)\n",
    "        ).to(device) \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        extended_input = self.ext_block(inputs)\n",
    "        convolved_input = self.block(inputs)\n",
    "        return convolved_input + extended_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(GenerativeNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = 4\n",
    "        self.patch_size = 16\n",
    "        configuration = ViTConfig(num_attention_heads=4, num_hidden_layers=8, hidden_size=self.hidden_size, patch_size=self.patch_size, num_channels=1, image_size=1024)\n",
    "        self.vit = ViTModel(configuration).to(self.device)\n",
    "        self.model = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(3,3), padding=0, stride=2), \n",
    "                        nn.GELU(), \n",
    "                        nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(3,3), padding=1, stride=1), \n",
    "                        nn.GELU(),\n",
    "                      \n",
    "                        ResidualBlock(),\n",
    "                        nn.GELU(),                      \n",
    "                        ResidualBlock(),\n",
    "                        nn.GELU(), \n",
    "                      \n",
    "                        nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=(2,2), padding=1, stride=1), \n",
    "                        nn.GELU()\n",
    "        ).to(device)\n",
    "        \n",
    "    \n",
    "    def patch_to_img(self, x, patch_size):\n",
    "        B, NumPatches, HiddenSize = x.shape\n",
    "        x = x.reshape(B, NumPatches, 1, HiddenSize)\n",
    "        x = x.reshape(B, NumPatches, 1, patch_size, patch_size)\n",
    "        x = x.permute(0, 1, 3, 4, 2)\n",
    "        x = x.reshape(B, int(math.sqrt(NumPatches)), int(math.sqrt(NumPatches)), patch_size, patch_size, 1)\n",
    "        x = x.permute(0,1,3,2,4,5)\n",
    "        new_h = x.shape[1] * x.shape[2]\n",
    "        new_w = x.shape[3] * x.shape[4]\n",
    "        x = x.reshape(B, new_h, new_w, 1)\n",
    "        x = x.swapaxes(3, 1)\n",
    "        x = x.swapaxes(3, 2)\n",
    "        return x\n",
    "    \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if inputs.device == 'cpu':\n",
    "            inputs = inputs.to(self.device)\n",
    "        vit_res = self.vit(pixel_values=inputs)\n",
    "        inputs = vit_res.last_hidden_state[:, 1:, :]\n",
    "        patch_size_after_vit = int(math.sqrt(inputs.shape[2]))\n",
    "        inputs = self.patch_to_img(inputs, patch_size_after_vit)\n",
    "        return self.model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminativeNetwork(nn.Module): \n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        super(DiscriminativeNetwork, self).__init__()\n",
    "        self.device = device\n",
    "        self.classifier = nn.Sequential(\n",
    "                                        nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3, stride=2),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.BatchNorm2d(2),\n",
    "                                        nn.Conv2d(in_channels=2, out_channels=4, kernel_size=3, stride=2),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.BatchNorm2d(4),\n",
    "                                        nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.BatchNorm2d(8),\n",
    "                                        nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.BatchNorm2d(16),\n",
    "                                        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.BatchNorm2d(32),\n",
    "                                        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2), #3x1\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.BatchNorm2d(64),\n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Dropout(0.3),\n",
    "                                        nn.Linear(in_features=14400, out_features=1024),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.Dropout(0.3),\n",
    "                                        nn.Linear(in_features=1024, out_features=128),\n",
    "                                        nn.LeakyReLU(0.2),\n",
    "                                        nn.Dropout(0.3),\n",
    "                                        nn.Linear(in_features=128, out_features=1),\n",
    "                                        nn.Sigmoid()\n",
    "                                        \n",
    "                                        \n",
    "        ).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if inputs.device == 'cpu':\n",
    "            inputs = inputs.to(self.device)\n",
    "        return self.classifier(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LHB_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path, ext):\n",
    "        self.path = path\n",
    "        self.ext = ext\n",
    "        self.len = len(os.listdir(self.path))\n",
    "        self.items_in_dir = os.listdir(self.path)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "       \n",
    "        name = self.path + '/' + self.items_in_dir[idx]\n",
    "\n",
    "        with open(name, 'rb') as fd:\n",
    "            song = pickle.load(fd)\n",
    "\n",
    "        return song[:1318970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './UnzippedDataset/train'\n",
    "\n",
    "train_ds = LHB_Dataset(train_path, 'mus')\n",
    "\n",
    "print(train_ds[0].shape)\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train_generator = torch.Generator(device='cpu')\n",
    "train_generator.manual_seed(13)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "                                            dataset=train_ds, \n",
    "                                            batch_size=3, \n",
    "                                            shuffle=True,\n",
    "                                            generator=train_generator\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "generator = GenerativeNetwork(device).to(device)\n",
    "discriminator = DiscriminativeNetwork(device).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_gen = AdamW(generator.parameters(), lr=1e-4, weight_decay=1e-4) \n",
    "optimizer_dis = torch.optim.Adam(discriminator.parameters(), lr=1e-7) \n",
    "\n",
    "# Loss\n",
    "loss_gen = nn.MSELoss()\n",
    "loss_dis = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def save_model(model, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    filename = path + '/generator_' + str(datetime.datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")) + '.pt'\n",
    "    torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, generator, discriminator, optimizer_gen, optimizer_dis, loss_gen, loss_dis, epoches=1, beta=1.0, device='cpu'): \n",
    "    \n",
    "    filename = 'reshapeAfterVit_V1.txt'\n",
    "    \n",
    "    alpha = 1.5\n",
    "    \n",
    "    NUM_COLS = 1024\n",
    "\n",
    "    # TrainSteps\n",
    "    for epoch in range(epoches):\n",
    "        print('EPOCH: ', epoch)\n",
    "        num_samples_seen = 0\n",
    "        total_gen_loss = 0\n",
    "        total_dis_loss = 0\n",
    "        \n",
    "        # Iter on batches\n",
    "        for data_batch in trainloader: \n",
    "            print('START BATCH PROCESS')\n",
    "            batch_lf = []\n",
    "            batch_hf = []\n",
    "            \n",
    "            for data in data_batch:\n",
    "                data = data.squeeze(dim=0) \n",
    "\n",
    "                \n",
    "                train_stft = librosa.stft(np.asarray(data), n_fft=4096, win_length=4096, window=signal.windows.hamming(4096))\n",
    "                train_spectrogram = torch.tensor(librosa.amplitude_to_db(abs(train_stft)))\n",
    "\n",
    "                rows = train_spectrogram.shape[0]\n",
    "                cols = train_spectrogram.shape[1]\n",
    "\n",
    "                    \n",
    "                train_spectrogram = train_spectrogram.reshape(1, rows, cols).float()\n",
    "\n",
    "                \n",
    "                batch_lf.append(train_spectrogram[:,1:1025,:1024])\n",
    "                batch_hf.append(train_spectrogram[:,1025:,:1024])\n",
    "\n",
    "                num_samples_seen += 1\n",
    "\n",
    "            \n",
    "             \n",
    "            batch_lf = torch.stack(batch_lf).to(device)\n",
    "            batch_hf = torch.stack(batch_hf).to(device)\n",
    "            \n",
    "            \n",
    "            shuffled_indexes = np.random.permutation(batch_lf.shape[0]) #shuffle\n",
    "            batch_lf = batch_lf[shuffled_indexes]\n",
    "            batch_hf = batch_hf[shuffled_indexes]\n",
    "                        \n",
    "            # Train the discriminator on the true/generated data\n",
    "            generated_data = generator(batch_lf)\n",
    "            combined_data = torch.cat((batch_hf.to(device), generated_data.detach()), dim=0)          \n",
    "            labels = torch.cat((torch.ones(batch_hf.shape[0]), torch.zeros(generated_data.shape[0])), dim=0)\n",
    "            \n",
    "            shuffled_indexes = np.random.permutation(combined_data.shape[0]) #shuffle\n",
    "            combined_data = combined_data[shuffled_indexes]\n",
    "            labels = labels[shuffled_indexes].to(device)\n",
    "\n",
    "            optimizer_dis.zero_grad()\n",
    "            discriminator_out = discriminator(combined_data).reshape(-1)\n",
    "            discriminator_loss = loss_dis(discriminator_out, labels)\n",
    "            print(\"Discriminator \"+str(discriminator_loss.item()))\n",
    "            discriminator_loss.backward()\n",
    "            optimizer_dis.step()\n",
    "            \n",
    "            # Train the generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            generator_out = generator(batch_lf)\n",
    "            generator_loss = loss_gen(batch_hf, generator_out)\n",
    "            \n",
    "            discriminator_out_gen = discriminator(generator_out).reshape(-1)\n",
    "            discriminator_loss_gen = loss_dis(discriminator_out_gen.to('cpu'), torch.ones(size=(discriminator_out_gen.shape[0],))) #bce\n",
    "                        \n",
    "            total_dis_loss = total_dis_loss + discriminator_loss_gen.detach()\n",
    "            total_gen_loss = total_gen_loss + generator_loss.detach()\n",
    "\n",
    "            print(\"Generator content \"+str(generator_loss.item()))\n",
    "            print(\"Generator adv \"+str(discriminator_loss_gen.item()))\n",
    "\n",
    "            loss = alpha*generator_loss + beta*discriminator_loss_gen \n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_gen.step()\n",
    "            \n",
    "        # End Trainloader Loop\n",
    "        \n",
    "\n",
    "        mean_gen_loss = total_gen_loss / num_samples_seen\n",
    "        mean_dis_loss = total_dis_loss / num_samples_seen\n",
    "\n",
    "        gen_order = torch.floor(torch.log10(mean_gen_loss))\n",
    "        dis_order = 0 if mean_dis_loss == 0 else torch.floor(torch.log10(mean_dis_loss))\n",
    "        b_pow = gen_order - dis_order \n",
    "        if b_pow > 0:\n",
    "            b_pow = b_pow\n",
    "        beta = pow(10.0, b_pow)\n",
    "        \n",
    "        save_model(generator)\n",
    "        file = open(filename, 'a')\n",
    "        file.write(\n",
    "        'EPOCH ' + str(epoch+1) +\n",
    "        '\\n\\t -> Discriminative Loss during D Training = ' + str(mean_dis_loss.item()) + ', during G Training = ' + str(discriminator_loss_gen.item()) +\n",
    "        '\\n\\t -> Generative Loss = ' + str(loss.item()) + ' ---> alpha * ' + str(mean_gen_loss.item()) + ' beta * ' + str(mean_dis_loss.item()))\n",
    "        file.flush()\n",
    "        file.close()     \n",
    "        \n",
    "        print('EPOCH ' + str(epoch+1) +\n",
    "        '\\n\\t -> Discriminative Loss during D Training = ' + str(mean_dis_loss.item()) + ', during G Training = ' + str(discriminator_loss_gen.item()) +\n",
    "        '\\n\\t -> Generative Loss = ' + str(loss.item()) + ' ---> alpha * ' + str(mean_gen_loss.item()) + ' beta * ' + str(mean_dis_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(trainloader, generator, discriminator, optimizer_gen, optimizer_dis, loss_gen, loss_dis, epoches=1, beta=1.0, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
